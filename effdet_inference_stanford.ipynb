{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "million-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-deps 'timm-0.1.26-py3-none-any.whl' > /dev/null\n",
    "# !pip install omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wanted-ghana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: timm 0.1.26\n",
      "Uninstalling timm-0.1.26:\n",
      "  Successfully uninstalled timm-0.1.26\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "social-tulsa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm==0.3.2\n",
      "  Using cached timm-0.3.2-py3-none-any.whl (244 kB)\n",
      "Requirement already satisfied: torch>=1.0 in /home/ad/venv/lib/python3.8/site-packages (from timm==0.3.2) (1.8.1+cu111)\n",
      "Requirement already satisfied: torchvision in /home/ad/venv/lib/python3.8/site-packages (from timm==0.3.2) (0.9.1+cu111)\n",
      "Requirement already satisfied: numpy in /home/ad/venv/lib/python3.8/site-packages (from torch>=1.0->timm==0.3.2) (1.20.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ad/venv/lib/python3.8/site-packages (from torch>=1.0->timm==0.3.2) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ad/venv/lib/python3.8/site-packages (from torchvision->timm==0.3.2) (8.1.2)\n",
      "Installing collected packages: timm\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 0.4.9\n",
      "    Uninstalling timm-0.4.9:\n",
      "      Successfully uninstalled timm-0.4.9\n",
      "Successfully installed timm-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install timm==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "occupational-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-deps 'timm-0.1.26-py3-none-any.whl' > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mathematical-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"timm-efficientdet-pytorch\")\n",
    "# sys.path.insert(0, \"../input/omegaconf\")\n",
    "sys.path.insert(0, \"effdet\")\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from glob import glob\n",
    "import pickle\n",
    "from effdet_old import get_efficientdet_config, EfficientDet, DetBenchEval\n",
    "from effdet_old.efficientdet import HeadNet\n",
    "from ensemble_boxes import *\n",
    "SEED = 42\n",
    "import gc\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "distinguished-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_codes = {\n",
    "\"Pedestrian\": 0,\n",
    "\"Biker\": 1,\n",
    "\"Car\": 2,\n",
    "\"Bus\": 3,\n",
    "\"Skater\": 4,\n",
    "\"Cart\": 5\n",
    "}\n",
    "label_colors = {\n",
    "0: (1,0,0),\n",
    "1:(0,1,0),\n",
    "2: (0,0,1),\n",
    " 3: (1,1,0),\n",
    "4:(0,1,1),\n",
    "5:(1,1,1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "advance-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('train_dataset.df', 'wb') as f:\n",
    "#     pickle.dump(train_dataset.df, f)\n",
    "# with open('train_dataset.df', 'rb') as f:\n",
    "#     df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-manhattan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "canadian-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms =  A.Compose([\n",
    "#                 A.RandomSizedCrop(min_max_height=(800, 800), height=1024, width=1024, p=1),\n",
    "            A.RandomCrop(512, 512),\n",
    "#             A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lucky-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, image_ids, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_ids = image_ids\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = transforms(image=image)\n",
    "            image = sample['image']\n",
    "        return image,image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "operating-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_PATH = '/mnt/r4/aliev/stanford_voc/VOC2012/JPEGImages'\n",
    "dataset = DatasetRetriever(\n",
    "    image_ids=np.array([path.split('/')[-1][:-4] for path in glob(f'{DATA_ROOT_PATH}/*.jpg')]),\n",
    "    transforms=transforms\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "removable-crowd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'FeatureInfo' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eaedaf98a3df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'effdet0_loss_055_state_dict.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-eaedaf98a3df>\u001b[0m in \u001b[0;36mload_net\u001b[0;34m(checkpoint_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_efficientdet_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tf_efficientdet_d0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEfficientDet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_backbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/semantic/effdet_old/efficientdet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, norm_kwargs, pretrained_backbone)\u001b[0m\n\u001b[1;32m    432\u001b[0m             pretrained=pretrained_backbone, **config.backbone_args)\n\u001b[1;32m    433\u001b[0m         feature_info = [dict(num_chs=f['num_chs'], reduction=f['reduction'])\n\u001b[0;32m--> 434\u001b[0;31m                         for i, f in enumerate(self.backbone.feature_info())]\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiFpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHeadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'FeatureInfo' object is not callable"
     ]
    }
   ],
   "source": [
    "def load_net(checkpoint_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d0')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "\n",
    "    config.num_classes = 6\n",
    "    config.image_size=512\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "\n",
    "    net = DetBenchEval(net, config)\n",
    "    net.eval();\n",
    "    return net.cuda()\n",
    "\n",
    "\n",
    "net = load_net('effdet0_loss_055_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "helpful-cable",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5491846e676b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "images = torch.stack(images).cuda().float()\n",
    "with torch.no_grad():\n",
    "        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adopted-excitement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5905e+02, 2.2716e+02, 1.4079e+01, 1.9785e+01, 3.7305e-02, 1.0000e+00],\n",
       "        [1.2764e+01, 4.3111e+02, 2.2411e+01, 2.4791e+01, 3.5872e-02, 1.0000e+00]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "personalized-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(images, score_threshold=0.12):\n",
    "    images = torch.stack(images).cuda().float()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n",
    "        for i in range(images.shape[0]):\n",
    "            boxes = det[i].detach().cpu().numpy()[:,:4]    \n",
    "            scores = det[i].detach().cpu().numpy()[:,4]\n",
    "            labels = det[i].detach().cpu().numpy()[:,5]\n",
    "            indexes = np.where(scores > score_threshold)[0]\n",
    "            boxes = boxes[indexes]\n",
    "            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "            predictions.append({\n",
    "                'boxes': boxes[indexes],\n",
    "                'scores': scores[indexes],\n",
    "                'labels': labels[indexes]\n",
    "            })\n",
    "    return [predictions],det\n",
    "\n",
    "def run_wbf(predictions, image_index, image_size=512, iou_thr=0.12, skip_box_thr=0.12, weights=None):\n",
    "    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n",
    "    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n",
    "    labels = [prediction[image_index]['labels'].tolist()  for prediction in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "inner-continuity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for j, (images,image_ids) in enumerate(data_loader):\n",
    "    break\n",
    "predictions,det = make_predictions(images)\n",
    "\n",
    "i = 0\n",
    "sample = images[i].permute(1,2,0).cpu().numpy()\n",
    "\n",
    "boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "boxes = boxes.astype(np.int32).clip(min=0, max=511)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 16))\n",
    "\n",
    "for box,label in zip(boxes,labels):\n",
    "    cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), label_colors[label], 1)\n",
    "    \n",
    "ax.set_axis_off()\n",
    "ax.imshow(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for j, t in enumerate(val_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net.forward(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.image_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.image_ids.index('coupa_video0_11128.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '/mnt/r4/aliev/stanford_voc/JPEGImages/bookstore_video1_4.jpg'\n",
    "image = cv2.imread(fname, cv2.IMREAD_COLOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
