{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alert-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"effdet\")\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from glob import glob\n",
    "import pickle\n",
    "from effdet_old import get_efficientdet_config, EfficientDet, DetBenchEval\n",
    "from effdet_old.efficientdet import HeadNet\n",
    "from ensemble_boxes import *\n",
    "SEED = 42\n",
    "import gc\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "forward-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_codes = {\n",
    "\"Pedestrian\": 0,\n",
    "\"Biker\": 1,\n",
    "\"Car\": 2,\n",
    "\"Bus\": 3,\n",
    "\"Skater\": 4,\n",
    "\"Cart\": 5\n",
    "}\n",
    "label_colors = {\n",
    "0: (255,0,0),\n",
    "1:(0,255,0),\n",
    "2: (0,0,255),\n",
    " 3: (255,255,0),\n",
    "4:(0,255,255),\n",
    "5:(255,255,255)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advisory-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms =  A.Compose([\n",
    "#                 A.RandomSizedCrop(min_max_height=(800, 800), height=1024, width=1024, p=1),\n",
    "#             A.RandomCrop(512, 512),\n",
    "#             A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "designed-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, image_ids, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_ids = image_ids\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = transforms(image=image)\n",
    "            image = sample['image']\n",
    "        return image,image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chronic-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_PATH = '/mnt/r4/aliev/stanford_voc/VOC2012/JPEGImages'\n",
    "dataset = DatasetRetriever(\n",
    "    image_ids=np.array([path.split('/')[-1][:-4] for path in glob(f'{DATA_ROOT_PATH}/*.jpg')]),\n",
    "    transforms=transforms\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rapid-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(checkpoint_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d0')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "\n",
    "    config.num_classes = 6\n",
    "    config.image_size=512\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "\n",
    "    net = DetBenchEval(net, config)\n",
    "    net.eval();\n",
    "    return net.cuda()\n",
    "\n",
    "\n",
    "net = load_net('effdet0_loss_055_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "confidential-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(batch, score_threshold=0.15):\n",
    "#     images = torch.stack(images).cuda().float()\n",
    "    batch = batch.cuda().float()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        det = net(batch, torch.tensor([1]*batch.shape[0]).float().cuda())\n",
    "\n",
    "        for i in range(batch.shape[0]):\n",
    "            boxes = det[i].detach().cpu().numpy()[:,:4]    \n",
    "            scores = det[i].detach().cpu().numpy()[:,4]\n",
    "            labels = det[i].detach().cpu().numpy()[:,5]\n",
    "            indexes = np.where(scores > score_threshold)[0]\n",
    "            boxes = boxes[indexes]\n",
    "            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "            predictions.append({\n",
    "                'boxes': boxes[indexes],\n",
    "                'scores': scores[indexes],\n",
    "                'labels': labels[indexes]\n",
    "            })\n",
    "    return predictions,det\n",
    "\n",
    "def run_wbf(predictions, image_size=512, iou_thr=0.15, skip_box_thr=0.15, weights=None):\n",
    "    boxes = [(prediction['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n",
    "    scores = [prediction['scores'].tolist()  for prediction in predictions]\n",
    "    labels = [prediction['labels'].tolist()  for prediction in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fitting-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "desperate-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "fname = '/mnt/r4/aliev/stanford_voc/VOC2012/JPEGImages/bookstore_video1_4.jpg'\n",
    "frame = cv2.imread(fname, cv2.IMREAD_COLOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = t(frame)\n",
    "batch = torch.zeros((4,3,512,512),dtype=torch.float32)\n",
    "batch[0] = image[:,:512,:512]\n",
    "batch[1] = image[:,512:1024,:512]\n",
    "batch[2] = image[:,:512,512:1024]\n",
    "batch[3] = image[:,512:1024,512:1024]\n",
    "batch = batch.cuda().float()\n",
    "predictions,det = make_predictions(batch)\n",
    "patch = [None,None,None,None]\n",
    "patch[0] = frame[:512,:512]\n",
    "patch[1] = frame[512:1024,:512]\n",
    "patch[2] = frame[:512,512:1024]\n",
    "patch[3] = frame[512:1024,512:1024]\n",
    "\n",
    "for i in range(4):\n",
    "    boxes, scores, labels = run_wbf([predictions[i]])\n",
    "    boxes = boxes.astype(np.int32).clip(min=0, max=511)\n",
    "    for box,label in zip(boxes,labels):\n",
    "        cv2.rectangle(patch[i], (box[0], box[1]), (box[2], box[3]), label_colors[label], 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 16))\n",
    "ax.set_axis_off()\n",
    "ax.imshow(frame)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
